{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "一个回归问题的建模实例\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/kaggle_house/train.csv')\n",
    "test_data = pd.read_csv('../data/kaggle_house/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n",
       "0   1          60       RL         65.0       WD        Normal     208500\n",
       "1   2          20       RL         80.0       WD        Normal     181500\n",
       "2   3          60       RL         68.0       WD        Normal     223500\n",
       "3   4          70       RL         60.0       WD       Abnorml     140000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值特征归一化，类别特征做one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std())) # apply的这种写法有点诡异，需要注意\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0) # 用0填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>-0.184443</td>\n",
       "      <td>-0.217841</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>1.046078</td>\n",
       "      <td>0.896679</td>\n",
       "      <td>0.523038</td>\n",
       "      <td>0.580708</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348780</td>\n",
       "      <td>-0.740634</td>\n",
       "      <td>0.199972</td>\n",
       "      <td>-0.359539</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.285886</td>\n",
       "      <td>-0.063139</td>\n",
       "      <td>-0.089577</td>\n",
       "      <td>-1.551918</td>\n",
       "      <td>0.157619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.873466</td>\n",
       "      <td>0.458096</td>\n",
       "      <td>-0.072032</td>\n",
       "      <td>-0.063174</td>\n",
       "      <td>2.187904</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>-0.395536</td>\n",
       "      <td>-0.569893</td>\n",
       "      <td>1.177709</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059772</td>\n",
       "      <td>1.614603</td>\n",
       "      <td>-0.702722</td>\n",
       "      <td>-0.359539</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.285886</td>\n",
       "      <td>-0.063139</td>\n",
       "      <td>-0.089577</td>\n",
       "      <td>-0.446848</td>\n",
       "      <td>-0.602858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>-0.055935</td>\n",
       "      <td>0.137173</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>0.980053</td>\n",
       "      <td>0.848819</td>\n",
       "      <td>0.333448</td>\n",
       "      <td>0.097840</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627338</td>\n",
       "      <td>-0.740634</td>\n",
       "      <td>-0.081195</td>\n",
       "      <td>-0.359539</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.285886</td>\n",
       "      <td>-0.063139</td>\n",
       "      <td>-0.089577</td>\n",
       "      <td>1.026577</td>\n",
       "      <td>0.157619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.302516</td>\n",
       "      <td>-0.398622</td>\n",
       "      <td>-0.078371</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>-1.859033</td>\n",
       "      <td>-0.682695</td>\n",
       "      <td>-0.569893</td>\n",
       "      <td>-0.494771</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785188</td>\n",
       "      <td>-0.740634</td>\n",
       "      <td>-0.184783</td>\n",
       "      <td>3.874303</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.285886</td>\n",
       "      <td>-0.063139</td>\n",
       "      <td>-0.089577</td>\n",
       "      <td>-1.551918</td>\n",
       "      <td>-1.363335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>0.629439</td>\n",
       "      <td>0.518814</td>\n",
       "      <td>1.355319</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>0.947040</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>1.381770</td>\n",
       "      <td>0.468770</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>1.685860</td>\n",
       "      <td>0.776834</td>\n",
       "      <td>0.540332</td>\n",
       "      <td>-0.359539</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.285886</td>\n",
       "      <td>-0.063139</td>\n",
       "      <td>-0.089577</td>\n",
       "      <td>2.131647</td>\n",
       "      <td>0.157619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.067320    -0.184443 -0.217841     0.646073    -0.507197   1.046078   \n",
       "1   -0.873466     0.458096 -0.072032    -0.063174     2.187904   0.154737   \n",
       "2    0.067320    -0.055935  0.137173     0.646073    -0.507197   0.980053   \n",
       "3    0.302516    -0.398622 -0.078371     0.646073    -0.507197  -1.859033   \n",
       "4    0.067320     0.629439  0.518814     1.355319    -0.507197   0.947040   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2    ...     GarageArea  \\\n",
       "0      0.896679    0.523038    0.580708    -0.29303    ...       0.348780   \n",
       "1     -0.395536   -0.569893    1.177709    -0.29303    ...      -0.059772   \n",
       "2      0.848819    0.333448    0.097840    -0.29303    ...       0.627338   \n",
       "3     -0.682695   -0.569893   -0.494771    -0.29303    ...       0.785188   \n",
       "4      0.753100    1.381770    0.468770    -0.29303    ...       1.685860   \n",
       "\n",
       "   WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "0   -0.740634     0.199972      -0.359539  -0.103313    -0.285886 -0.063139   \n",
       "1    1.614603    -0.702722      -0.359539  -0.103313    -0.285886 -0.063139   \n",
       "2   -0.740634    -0.081195      -0.359539  -0.103313    -0.285886 -0.063139   \n",
       "3   -0.740634    -0.184783       3.874303  -0.103313    -0.285886 -0.063139   \n",
       "4    0.776834     0.540332      -0.359539  -0.103313    -0.285886 -0.063139   \n",
       "\n",
       "    MiscVal    MoSold    YrSold  \n",
       "0 -0.089577 -1.551918  0.157619  \n",
       "1 -0.089577 -0.446848 -0.602858  \n",
       "2 -0.089577  1.026577  0.157619  \n",
       "3 -0.089577 -1.551918 -1.363335  \n",
       "4 -0.089577  2.131647  0.157619  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features[numeric_features][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 79)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot\n",
    "all_features = pd.get_dummies(all_features, dummy_na=True) # dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 331)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>-0.184443</td>\n",
       "      <td>-0.217841</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>1.046078</td>\n",
       "      <td>0.896679</td>\n",
       "      <td>0.523038</td>\n",
       "      <td>0.580708</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.873466</td>\n",
       "      <td>0.458096</td>\n",
       "      <td>-0.072032</td>\n",
       "      <td>-0.063174</td>\n",
       "      <td>2.187904</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>-0.395536</td>\n",
       "      <td>-0.569893</td>\n",
       "      <td>1.177709</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>-0.055935</td>\n",
       "      <td>0.137173</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>0.980053</td>\n",
       "      <td>0.848819</td>\n",
       "      <td>0.333448</td>\n",
       "      <td>0.097840</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.302516</td>\n",
       "      <td>-0.398622</td>\n",
       "      <td>-0.078371</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>-1.859033</td>\n",
       "      <td>-0.682695</td>\n",
       "      <td>-0.569893</td>\n",
       "      <td>-0.494771</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>0.629439</td>\n",
       "      <td>0.518814</td>\n",
       "      <td>1.355319</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>0.947040</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>1.381770</td>\n",
       "      <td>0.468770</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.067320    -0.184443 -0.217841     0.646073    -0.507197   1.046078   \n",
       "1   -0.873466     0.458096 -0.072032    -0.063174     2.187904   0.154737   \n",
       "2    0.067320    -0.055935  0.137173     0.646073    -0.507197   0.980053   \n",
       "3    0.302516    -0.398622 -0.078371     0.646073    -0.507197  -1.859033   \n",
       "4    0.067320     0.629439  0.518814     1.355319    -0.507197   0.947040   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2        ...          \\\n",
       "0      0.896679    0.523038    0.580708    -0.29303        ...           \n",
       "1     -0.395536   -0.569893    1.177709    -0.29303        ...           \n",
       "2      0.848819    0.333448    0.097840    -0.29303        ...           \n",
       "3     -0.682695   -0.569893   -0.494771    -0.29303        ...           \n",
       "4      0.753100    1.381770    0.468770    -0.29303        ...           \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_Abnorml  \\\n",
       "0             0            1             0                      0   \n",
       "1             0            1             0                      0   \n",
       "2             0            1             0                      0   \n",
       "3             0            1             0                      1   \n",
       "4             0            1             0                      0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                      0                     0                     0   \n",
       "1                      0                     0                     0   \n",
       "2                      0                     0                     0   \n",
       "3                      0                     0                     0   \n",
       "4                      0                     0                     0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  SaleCondition_nan  \n",
       "0                     1                      0                  0  \n",
       "1                     1                      0                  0  \n",
       "2                     1                      0                  0  \n",
       "3                     0                      0                  0  \n",
       "4                     1                      0                  0  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把train和test分开，加上label\n",
    "n_train = train_data.shape[0]\n",
    "train_features_all = np.array(all_features[:n_train].values,dtype=np.float)\n",
    "test_features = np.array(all_features[n_train:].values,dtype=np.float)\n",
    "train_labels_all = np.array(train_data.SalePrice.values.reshape(-1, 1),dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[983, 426, 735, 1174, 952]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_shuffle = [x for x in range(n_train)]\n",
    "np.random.shuffle(idx_shuffle)\n",
    "idx_shuffle[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num = int(np.floor(n_train * 0.8))\n",
    "train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = train_features_all[train_num:]\n",
    "valid_labels = train_labels_all[train_num:]\n",
    "train_features = train_features_all[:train_num]\n",
    "train_labels = train_labels_all[:train_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)\n",
    "                                    ]) # 回归问题是这么设置的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.mean_squared_logarithmic_error, \n",
    "              optimizer=tf.keras.optimizers.Adam(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1168/1168 [==============================] - 1s 530us/sample - loss: 83.7847 - val_loss: 65.8267\n",
      "Epoch 2/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 62.8248 - val_loss: 59.9730\n",
      "Epoch 3/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 58.5891 - val_loss: 57.2218\n",
      "Epoch 4/500\n",
      "1168/1168 [==============================] - 0s 43us/sample - loss: 56.3119 - val_loss: 55.4553\n",
      "Epoch 5/500\n",
      "1168/1168 [==============================] - 0s 46us/sample - loss: 54.7394 - val_loss: 54.1060\n",
      "Epoch 6/500\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 53.4834 - val_loss: 52.9741\n",
      "Epoch 7/500\n",
      "1168/1168 [==============================] - 0s 52us/sample - loss: 52.4078 - val_loss: 51.9758\n",
      "Epoch 8/500\n",
      "1168/1168 [==============================] - 0s 46us/sample - loss: 51.4438 - val_loss: 51.0720\n",
      "Epoch 9/500\n",
      "1168/1168 [==============================] - 0s 50us/sample - loss: 50.5680 - val_loss: 50.2377\n",
      "Epoch 10/500\n",
      "1168/1168 [==============================] - 0s 59us/sample - loss: 49.7621 - val_loss: 49.4667\n",
      "Epoch 11/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 49.0165 - val_loss: 48.7548\n",
      "Epoch 12/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 48.3232 - val_loss: 48.0898\n",
      "Epoch 13/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 47.6761 - val_loss: 47.4666\n",
      "Epoch 14/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 47.0693 - val_loss: 46.8847\n",
      "Epoch 15/500\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 46.4998 - val_loss: 46.3357\n",
      "Epoch 16/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 45.9619 - val_loss: 45.8174\n",
      "Epoch 17/500\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 45.4548 - val_loss: 45.3273\n",
      "Epoch 18/500\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 44.9747 - val_loss: 44.8628\n",
      "Epoch 19/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 44.5180 - val_loss: 44.4190\n",
      "Epoch 20/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 44.0836 - val_loss: 43.9952\n",
      "Epoch 21/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 43.6687 - val_loss: 43.5936\n",
      "Epoch 22/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 43.2732 - val_loss: 43.2085\n",
      "Epoch 23/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 42.8941 - val_loss: 42.8393\n",
      "Epoch 24/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 42.5308 - val_loss: 42.4857\n",
      "Epoch 25/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 42.1821 - val_loss: 42.1461\n",
      "Epoch 26/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 41.8469 - val_loss: 41.8177\n",
      "Epoch 27/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 41.5239 - val_loss: 41.5005\n",
      "Epoch 28/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 41.2121 - val_loss: 41.1951\n",
      "Epoch 29/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 40.9111 - val_loss: 40.8990\n",
      "Epoch 30/500\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 40.6203 - val_loss: 40.6131\n",
      "Epoch 31/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 40.3393 - val_loss: 40.3352\n",
      "Epoch 32/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 40.0666 - val_loss: 40.0678\n",
      "Epoch 33/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 39.8027 - val_loss: 39.8085\n",
      "Epoch 34/500\n",
      "1168/1168 [==============================] - 0s 45us/sample - loss: 39.5470 - val_loss: 39.5567\n",
      "Epoch 35/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 39.2987 - val_loss: 39.3111\n",
      "Epoch 36/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 39.0572 - val_loss: 39.0739\n",
      "Epoch 37/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 38.8228 - val_loss: 38.8431\n",
      "Epoch 38/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 38.5947 - val_loss: 38.6195\n",
      "Epoch 39/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 38.3727 - val_loss: 38.3995\n",
      "Epoch 40/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 38.1563 - val_loss: 38.1849\n",
      "Epoch 41/500\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 37.9453 - val_loss: 37.9776\n",
      "Epoch 42/500\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 37.7396 - val_loss: 37.7755\n",
      "Epoch 43/500\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 37.5391 - val_loss: 37.5771\n",
      "Epoch 44/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 37.3431 - val_loss: 37.3827\n",
      "Epoch 45/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 37.1514 - val_loss: 37.1916\n",
      "Epoch 46/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 36.9638 - val_loss: 37.0065\n",
      "Epoch 47/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 36.7806 - val_loss: 36.8259\n",
      "Epoch 48/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 36.6013 - val_loss: 36.6492\n",
      "Epoch 49/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 36.4259 - val_loss: 36.4758\n",
      "Epoch 50/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 36.2540 - val_loss: 36.3056\n",
      "Epoch 51/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 36.0857 - val_loss: 36.1392\n",
      "Epoch 52/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 35.9209 - val_loss: 35.9756\n",
      "Epoch 53/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 35.7591 - val_loss: 35.8154\n",
      "Epoch 54/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 35.6007 - val_loss: 35.6571\n",
      "Epoch 55/500\n",
      "1168/1168 [==============================] - 0s 45us/sample - loss: 35.4449 - val_loss: 35.5026\n",
      "Epoch 56/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 35.2922 - val_loss: 35.3512\n",
      "Epoch 57/500\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 35.1421 - val_loss: 35.2033\n",
      "Epoch 58/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 34.9951 - val_loss: 35.0576\n",
      "Epoch 59/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 34.8504 - val_loss: 34.9140\n",
      "Epoch 60/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 34.7083 - val_loss: 34.7736\n",
      "Epoch 61/500\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 34.5687 - val_loss: 34.6345\n",
      "Epoch 62/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 34.4312 - val_loss: 34.4987\n",
      "Epoch 63/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 34.2961 - val_loss: 34.3652\n",
      "Epoch 64/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 34.1634 - val_loss: 34.2337\n",
      "Epoch 65/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 34.0328 - val_loss: 34.1039\n",
      "Epoch 66/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 33.9043 - val_loss: 33.9768\n",
      "Epoch 67/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 33.7777 - val_loss: 33.8511\n",
      "Epoch 68/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 33.6523 - val_loss: 33.7257\n",
      "Epoch 69/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 33.5295 - val_loss: 33.6029\n",
      "Epoch 70/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 33.4084 - val_loss: 33.4826\n",
      "Epoch 71/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 33.2891 - val_loss: 33.3635\n",
      "Epoch 72/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 33.1714 - val_loss: 33.2462\n",
      "Epoch 73/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 33.0553 - val_loss: 33.1304\n",
      "Epoch 74/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 32.9408 - val_loss: 33.0167\n",
      "Epoch 75/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 32.8280 - val_loss: 32.9042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 32.7169 - val_loss: 32.7940\n",
      "Epoch 77/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 32.6072 - val_loss: 32.6851\n",
      "Epoch 78/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 32.4988 - val_loss: 32.5772\n",
      "Epoch 79/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 32.3920 - val_loss: 32.4713\n",
      "Epoch 80/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 32.2864 - val_loss: 32.3664\n",
      "Epoch 81/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 32.1820 - val_loss: 32.2625\n",
      "Epoch 82/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 32.0792 - val_loss: 32.1604\n",
      "Epoch 83/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 31.9776 - val_loss: 32.0591\n",
      "Epoch 84/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 31.8772 - val_loss: 31.9591\n",
      "Epoch 85/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 31.7779 - val_loss: 31.8607\n",
      "Epoch 86/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 31.6799 - val_loss: 31.7632\n",
      "Epoch 87/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 31.5831 - val_loss: 31.6671\n",
      "Epoch 88/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 31.4875 - val_loss: 31.5719\n",
      "Epoch 89/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 31.3929 - val_loss: 31.4777\n",
      "Epoch 90/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 31.2991 - val_loss: 31.3842\n",
      "Epoch 91/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 31.2066 - val_loss: 31.2914\n",
      "Epoch 92/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 31.1152 - val_loss: 31.2001\n",
      "Epoch 93/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 31.0248 - val_loss: 31.1103\n",
      "Epoch 94/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 30.9355 - val_loss: 31.0212\n",
      "Epoch 95/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 30.8470 - val_loss: 30.9332\n",
      "Epoch 96/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 30.7595 - val_loss: 30.8460\n",
      "Epoch 97/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 30.6728 - val_loss: 30.7595\n",
      "Epoch 98/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 30.5870 - val_loss: 30.6739\n",
      "Epoch 99/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 30.5022 - val_loss: 30.5892\n",
      "Epoch 100/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 30.4181 - val_loss: 30.5053\n",
      "Epoch 101/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 30.3349 - val_loss: 30.4217\n",
      "Epoch 102/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 30.2525 - val_loss: 30.3399\n",
      "Epoch 103/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 30.1711 - val_loss: 30.2587\n",
      "Epoch 104/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 30.0902 - val_loss: 30.1784\n",
      "Epoch 105/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 30.0104 - val_loss: 30.0986\n",
      "Epoch 106/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 29.9310 - val_loss: 30.0196\n",
      "Epoch 107/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 29.8527 - val_loss: 29.9414\n",
      "Epoch 108/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 29.7750 - val_loss: 29.8637\n",
      "Epoch 109/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 29.6980 - val_loss: 29.7871\n",
      "Epoch 110/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 29.6218 - val_loss: 29.7112\n",
      "Epoch 111/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 29.5462 - val_loss: 29.6357\n",
      "Epoch 112/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 29.4714 - val_loss: 29.5607\n",
      "Epoch 113/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 29.3972 - val_loss: 29.4867\n",
      "Epoch 114/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 29.3235 - val_loss: 29.4133\n",
      "Epoch 115/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 29.2505 - val_loss: 29.3405\n",
      "Epoch 116/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 29.1782 - val_loss: 29.2679\n",
      "Epoch 117/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 29.1064 - val_loss: 29.1963\n",
      "Epoch 118/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 29.0353 - val_loss: 29.1255\n",
      "Epoch 119/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 28.9649 - val_loss: 29.0552\n",
      "Epoch 120/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 28.8950 - val_loss: 28.9856\n",
      "Epoch 121/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 28.8258 - val_loss: 28.9166\n",
      "Epoch 122/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 28.7570 - val_loss: 28.8482\n",
      "Epoch 123/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 28.6889 - val_loss: 28.7802\n",
      "Epoch 124/500\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 28.6214 - val_loss: 28.7128\n",
      "Epoch 125/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 28.5544 - val_loss: 28.6461\n",
      "Epoch 126/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 28.4879 - val_loss: 28.5800\n",
      "Epoch 127/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 28.4219 - val_loss: 28.5141\n",
      "Epoch 128/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 28.3564 - val_loss: 28.4488\n",
      "Epoch 129/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 28.2914 - val_loss: 28.3841\n",
      "Epoch 130/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 28.2269 - val_loss: 28.3197\n",
      "Epoch 131/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 28.1628 - val_loss: 28.2559\n",
      "Epoch 132/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 28.0993 - val_loss: 28.1927\n",
      "Epoch 133/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 28.0363 - val_loss: 28.1300\n",
      "Epoch 134/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 27.9738 - val_loss: 28.0678\n",
      "Epoch 135/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 27.9117 - val_loss: 28.0058\n",
      "Epoch 136/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 27.8500 - val_loss: 27.9442\n",
      "Epoch 137/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 27.7889 - val_loss: 27.8832\n",
      "Epoch 138/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 27.7280 - val_loss: 27.8225\n",
      "Epoch 139/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 27.6675 - val_loss: 27.7620\n",
      "Epoch 140/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 27.6077 - val_loss: 27.7023\n",
      "Epoch 141/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 27.5481 - val_loss: 27.6428\n",
      "Epoch 142/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 27.4890 - val_loss: 27.5836\n",
      "Epoch 143/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 27.4302 - val_loss: 27.5250\n",
      "Epoch 144/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 27.3719 - val_loss: 27.4665\n",
      "Epoch 145/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 27.3138 - val_loss: 27.4087\n",
      "Epoch 146/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 27.2563 - val_loss: 27.3512\n",
      "Epoch 147/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 27.1991 - val_loss: 27.2941\n",
      "Epoch 148/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 27.1423 - val_loss: 27.2373\n",
      "Epoch 149/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 27.0859 - val_loss: 27.1811\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 29us/sample - loss: 27.0299 - val_loss: 27.1251\n",
      "Epoch 151/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 26.9743 - val_loss: 27.0694\n",
      "Epoch 152/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.9188 - val_loss: 27.0138\n",
      "Epoch 153/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.8639 - val_loss: 26.9588\n",
      "Epoch 154/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.8092 - val_loss: 26.9039\n",
      "Epoch 155/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.7549 - val_loss: 26.8494\n",
      "Epoch 156/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 26.7008 - val_loss: 26.7953\n",
      "Epoch 157/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 26.6472 - val_loss: 26.7415\n",
      "Epoch 158/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.5939 - val_loss: 26.6881\n",
      "Epoch 159/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 26.5408 - val_loss: 26.6351\n",
      "Epoch 160/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 26.4882 - val_loss: 26.5825\n",
      "Epoch 161/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 26.4358 - val_loss: 26.5301\n",
      "Epoch 162/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 26.3838 - val_loss: 26.4781\n",
      "Epoch 163/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 26.3320 - val_loss: 26.4266\n",
      "Epoch 164/500\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 26.2807 - val_loss: 26.3753\n",
      "Epoch 165/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 26.2295 - val_loss: 26.3245\n",
      "Epoch 166/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 26.1787 - val_loss: 26.2739\n",
      "Epoch 167/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 26.1283 - val_loss: 26.2236\n",
      "Epoch 168/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.0781 - val_loss: 26.1733\n",
      "Epoch 169/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 26.0280 - val_loss: 26.1235\n",
      "Epoch 170/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 25.9784 - val_loss: 26.0738\n",
      "Epoch 171/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 25.9290 - val_loss: 26.0242\n",
      "Epoch 172/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 25.8797 - val_loss: 25.9749\n",
      "Epoch 173/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 25.8308 - val_loss: 25.9259\n",
      "Epoch 174/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 25.7821 - val_loss: 25.8776\n",
      "Epoch 175/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 25.7339 - val_loss: 25.8295\n",
      "Epoch 176/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 25.6859 - val_loss: 25.7813\n",
      "Epoch 177/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 25.6380 - val_loss: 25.7334\n",
      "Epoch 178/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 25.5905 - val_loss: 25.6860\n",
      "Epoch 179/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 25.5432 - val_loss: 25.6388\n",
      "Epoch 180/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 25.4961 - val_loss: 25.5917\n",
      "Epoch 181/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 25.4494 - val_loss: 25.5449\n",
      "Epoch 182/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 25.4028 - val_loss: 25.4985\n",
      "Epoch 183/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 25.3565 - val_loss: 25.4520\n",
      "Epoch 184/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 25.3104 - val_loss: 25.4060\n",
      "Epoch 185/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 25.2645 - val_loss: 25.3604\n",
      "Epoch 186/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 25.2189 - val_loss: 25.3147\n",
      "Epoch 187/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 25.1735 - val_loss: 25.2694\n",
      "Epoch 188/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 25.1283 - val_loss: 25.2242\n",
      "Epoch 189/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 25.0833 - val_loss: 25.1793\n",
      "Epoch 190/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 25.0386 - val_loss: 25.1346\n",
      "Epoch 191/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 24.9940 - val_loss: 25.0899\n",
      "Epoch 192/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 24.9496 - val_loss: 25.0456\n",
      "Epoch 193/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 24.9055 - val_loss: 25.0015\n",
      "Epoch 194/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 24.8616 - val_loss: 24.9578\n",
      "Epoch 195/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 24.8179 - val_loss: 24.9141\n",
      "Epoch 196/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 24.7744 - val_loss: 24.8708\n",
      "Epoch 197/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 24.7312 - val_loss: 24.8276\n",
      "Epoch 198/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 24.6882 - val_loss: 24.7848\n",
      "Epoch 199/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 24.6454 - val_loss: 24.7421\n",
      "Epoch 200/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 24.6027 - val_loss: 24.6996\n",
      "Epoch 201/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 24.5603 - val_loss: 24.6569\n",
      "Epoch 202/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 24.5180 - val_loss: 24.6145\n",
      "Epoch 203/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 24.4759 - val_loss: 24.5724\n",
      "Epoch 204/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 24.4341 - val_loss: 24.5307\n",
      "Epoch 205/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 24.3925 - val_loss: 24.4891\n",
      "Epoch 206/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 24.3510 - val_loss: 24.4476\n",
      "Epoch 207/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 24.3095 - val_loss: 24.4061\n",
      "Epoch 208/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 24.2684 - val_loss: 24.3647\n",
      "Epoch 209/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 24.2273 - val_loss: 24.3239\n",
      "Epoch 210/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 24.1866 - val_loss: 24.2833\n",
      "Epoch 211/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 24.1460 - val_loss: 24.2428\n",
      "Epoch 212/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 24.1056 - val_loss: 24.2026\n",
      "Epoch 213/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 24.0653 - val_loss: 24.1623\n",
      "Epoch 214/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 24.0253 - val_loss: 24.1224\n",
      "Epoch 215/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.9854 - val_loss: 24.0826\n",
      "Epoch 216/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 23.9457 - val_loss: 24.0430\n",
      "Epoch 217/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 23.9061 - val_loss: 24.0035\n",
      "Epoch 218/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 23.8667 - val_loss: 23.9640\n",
      "Epoch 219/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 23.8274 - val_loss: 23.9243\n",
      "Epoch 220/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 23.7883 - val_loss: 23.8850\n",
      "Epoch 221/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 23.7493 - val_loss: 23.8461\n",
      "Epoch 222/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 23.7106 - val_loss: 23.8073\n",
      "Epoch 223/500\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 23.6719 - val_loss: 23.7685\n",
      "Epoch 224/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 40us/sample - loss: 23.6334 - val_loss: 23.7300\n",
      "Epoch 225/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 23.5951 - val_loss: 23.6916\n",
      "Epoch 226/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 23.5570 - val_loss: 23.6534\n",
      "Epoch 227/500\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 23.5189 - val_loss: 23.6156\n",
      "Epoch 228/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 23.4811 - val_loss: 23.5777\n",
      "Epoch 229/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 23.4434 - val_loss: 23.5400\n",
      "Epoch 230/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.4058 - val_loss: 23.5024\n",
      "Epoch 231/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 23.3683 - val_loss: 23.4649\n",
      "Epoch 232/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 23.3311 - val_loss: 23.4275\n",
      "Epoch 233/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.2939 - val_loss: 23.3904\n",
      "Epoch 234/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 23.2570 - val_loss: 23.3535\n",
      "Epoch 235/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.2201 - val_loss: 23.3164\n",
      "Epoch 236/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 23.1834 - val_loss: 23.2797\n",
      "Epoch 237/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.1467 - val_loss: 23.2430\n",
      "Epoch 238/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.1102 - val_loss: 23.2065\n",
      "Epoch 239/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 23.0740 - val_loss: 23.1701\n",
      "Epoch 240/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 23.0378 - val_loss: 23.1338\n",
      "Epoch 241/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 23.0017 - val_loss: 23.0977\n",
      "Epoch 242/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.9657 - val_loss: 23.0617\n",
      "Epoch 243/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.9299 - val_loss: 23.0258\n",
      "Epoch 244/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 22.8943 - val_loss: 22.9901\n",
      "Epoch 245/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.8588 - val_loss: 22.9545\n",
      "Epoch 246/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 22.8233 - val_loss: 22.9192\n",
      "Epoch 247/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.7881 - val_loss: 22.8841\n",
      "Epoch 248/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.7530 - val_loss: 22.8491\n",
      "Epoch 249/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.7180 - val_loss: 22.8141\n",
      "Epoch 250/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.6831 - val_loss: 22.7792\n",
      "Epoch 251/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.6483 - val_loss: 22.7443\n",
      "Epoch 252/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.6136 - val_loss: 22.7093\n",
      "Epoch 253/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.5790 - val_loss: 22.6746\n",
      "Epoch 254/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.5446 - val_loss: 22.6401\n",
      "Epoch 255/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.5103 - val_loss: 22.6057\n",
      "Epoch 256/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.4762 - val_loss: 22.5714\n",
      "Epoch 257/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 22.4420 - val_loss: 22.5373\n",
      "Epoch 258/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.4080 - val_loss: 22.5033\n",
      "Epoch 259/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.3741 - val_loss: 22.4694\n",
      "Epoch 260/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.3404 - val_loss: 22.4356\n",
      "Epoch 261/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.3067 - val_loss: 22.4020\n",
      "Epoch 262/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.2732 - val_loss: 22.3686\n",
      "Epoch 263/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.2398 - val_loss: 22.3350\n",
      "Epoch 264/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 22.2065 - val_loss: 22.3017\n",
      "Epoch 265/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.1733 - val_loss: 22.2685\n",
      "Epoch 266/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.1402 - val_loss: 22.2353\n",
      "Epoch 267/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.1071 - val_loss: 22.2023\n",
      "Epoch 268/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.0742 - val_loss: 22.1693\n",
      "Epoch 269/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 22.0414 - val_loss: 22.1366\n",
      "Epoch 270/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 22.0088 - val_loss: 22.1039\n",
      "Epoch 271/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.9762 - val_loss: 22.0712\n",
      "Epoch 272/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 21.9437 - val_loss: 22.0388\n",
      "Epoch 273/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.9113 - val_loss: 22.0063\n",
      "Epoch 274/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 21.8791 - val_loss: 21.9740\n",
      "Epoch 275/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 21.8469 - val_loss: 21.9417\n",
      "Epoch 276/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 21.8147 - val_loss: 21.9095\n",
      "Epoch 277/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 21.7827 - val_loss: 21.8775\n",
      "Epoch 278/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.7508 - val_loss: 21.8456\n",
      "Epoch 279/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 21.7192 - val_loss: 21.8138\n",
      "Epoch 280/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 21.6875 - val_loss: 21.7822\n",
      "Epoch 281/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 21.6559 - val_loss: 21.7507\n",
      "Epoch 282/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 21.6245 - val_loss: 21.7192\n",
      "Epoch 283/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 21.5931 - val_loss: 21.6879\n",
      "Epoch 284/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 21.5618 - val_loss: 21.6565\n",
      "Epoch 285/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 21.5305 - val_loss: 21.6252\n",
      "Epoch 286/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 21.4994 - val_loss: 21.5940\n",
      "Epoch 287/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 21.4684 - val_loss: 21.5628\n",
      "Epoch 288/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 21.4375 - val_loss: 21.5319\n",
      "Epoch 289/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 21.4066 - val_loss: 21.5010\n",
      "Epoch 290/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 21.3758 - val_loss: 21.4701\n",
      "Epoch 291/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.3452 - val_loss: 21.4394\n",
      "Epoch 292/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 21.3144 - val_loss: 21.4088\n",
      "Epoch 293/500\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 21.2839 - val_loss: 21.3782\n",
      "Epoch 294/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 21.2535 - val_loss: 21.3476\n",
      "Epoch 295/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.2231 - val_loss: 21.3170\n",
      "Epoch 296/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 21.1928 - val_loss: 21.2866\n",
      "Epoch 297/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.1625 - val_loss: 21.2563\n",
      "Epoch 298/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 33us/sample - loss: 21.1324 - val_loss: 21.2261\n",
      "Epoch 299/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 21.1024 - val_loss: 21.1960\n",
      "Epoch 300/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 21.0724 - val_loss: 21.1661\n",
      "Epoch 301/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 21.0425 - val_loss: 21.1363\n",
      "Epoch 302/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 21.0127 - val_loss: 21.1065\n",
      "Epoch 303/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 20.9830 - val_loss: 21.0768\n",
      "Epoch 304/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 20.9535 - val_loss: 21.0471\n",
      "Epoch 305/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.9239 - val_loss: 21.0177\n",
      "Epoch 306/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 20.8946 - val_loss: 20.9883\n",
      "Epoch 307/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 20.8652 - val_loss: 20.9590\n",
      "Epoch 308/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 20.8358 - val_loss: 20.9298\n",
      "Epoch 309/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 20.8066 - val_loss: 20.9005\n",
      "Epoch 310/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 20.7775 - val_loss: 20.8715\n",
      "Epoch 311/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 20.7484 - val_loss: 20.8424\n",
      "Epoch 312/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 20.7194 - val_loss: 20.8134\n",
      "Epoch 313/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 20.6905 - val_loss: 20.7844\n",
      "Epoch 314/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 20.6617 - val_loss: 20.7555\n",
      "Epoch 315/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 20.6329 - val_loss: 20.7265\n",
      "Epoch 316/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 20.6042 - val_loss: 20.6977\n",
      "Epoch 317/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.5756 - val_loss: 20.6689\n",
      "Epoch 318/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.5470 - val_loss: 20.6402\n",
      "Epoch 319/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 20.5185 - val_loss: 20.6117\n",
      "Epoch 320/500\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 20.4902 - val_loss: 20.5833\n",
      "Epoch 321/500\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 20.4618 - val_loss: 20.5548\n",
      "Epoch 322/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.4335 - val_loss: 20.5264\n",
      "Epoch 323/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 20.4053 - val_loss: 20.4980\n",
      "Epoch 324/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 20.3771 - val_loss: 20.4698\n",
      "Epoch 325/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 20.3490 - val_loss: 20.4417\n",
      "Epoch 326/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 20.3210 - val_loss: 20.4137\n",
      "Epoch 327/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 20.2932 - val_loss: 20.3857\n",
      "Epoch 328/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 20.2654 - val_loss: 20.3577\n",
      "Epoch 329/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 20.2375 - val_loss: 20.3299\n",
      "Epoch 330/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.2098 - val_loss: 20.3021\n",
      "Epoch 331/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 20.1821 - val_loss: 20.2745\n",
      "Epoch 332/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.1545 - val_loss: 20.2468\n",
      "Epoch 333/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 20.1270 - val_loss: 20.2193\n",
      "Epoch 334/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 20.0995 - val_loss: 20.1919\n",
      "Epoch 335/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 20.0722 - val_loss: 20.1645\n",
      "Epoch 336/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 20.0448 - val_loss: 20.1372\n",
      "Epoch 337/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 20.0176 - val_loss: 20.1099\n",
      "Epoch 338/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 19.9905 - val_loss: 20.0828\n",
      "Epoch 339/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 19.9634 - val_loss: 20.0557\n",
      "Epoch 340/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 19.9362 - val_loss: 20.0285\n",
      "Epoch 341/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 19.9093 - val_loss: 20.0014\n",
      "Epoch 342/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 19.8823 - val_loss: 19.9744\n",
      "Epoch 343/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 19.8554 - val_loss: 19.9475\n",
      "Epoch 344/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 19.8286 - val_loss: 19.9207\n",
      "Epoch 345/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 19.8018 - val_loss: 19.8940\n",
      "Epoch 346/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 19.7751 - val_loss: 19.8673\n",
      "Epoch 347/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 19.7484 - val_loss: 19.8406\n",
      "Epoch 348/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 19.7219 - val_loss: 19.8140\n",
      "Epoch 349/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 19.6953 - val_loss: 19.7873\n",
      "Epoch 350/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 19.6689 - val_loss: 19.7608\n",
      "Epoch 351/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 19.6424 - val_loss: 19.7343\n",
      "Epoch 352/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 19.6161 - val_loss: 19.7078\n",
      "Epoch 353/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 19.5898 - val_loss: 19.6814\n",
      "Epoch 354/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 19.5635 - val_loss: 19.6551\n",
      "Epoch 355/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 19.5374 - val_loss: 19.6290\n",
      "Epoch 356/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 19.5113 - val_loss: 19.6028\n",
      "Epoch 357/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 19.4852 - val_loss: 19.5767\n",
      "Epoch 358/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 19.4593 - val_loss: 19.5504\n",
      "Epoch 359/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 19.4333 - val_loss: 19.5244\n",
      "Epoch 360/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 19.4075 - val_loss: 19.4985\n",
      "Epoch 361/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 19.3817 - val_loss: 19.4725\n",
      "Epoch 362/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 19.3558 - val_loss: 19.4465\n",
      "Epoch 363/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 19.3301 - val_loss: 19.4207\n",
      "Epoch 364/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 19.3044 - val_loss: 19.3950\n",
      "Epoch 365/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 19.2788 - val_loss: 19.3694\n",
      "Epoch 366/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 19.2533 - val_loss: 19.3438\n",
      "Epoch 367/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 19.2278 - val_loss: 19.3183\n",
      "Epoch 368/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 19.2023 - val_loss: 19.2927\n",
      "Epoch 369/500\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 19.1769 - val_loss: 19.2674\n",
      "Epoch 370/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 19.1515 - val_loss: 19.2420\n",
      "Epoch 371/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 19.1263 - val_loss: 19.2168\n",
      "Epoch 372/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 33us/sample - loss: 19.1012 - val_loss: 19.1914\n",
      "Epoch 373/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 19.0759 - val_loss: 19.1663\n",
      "Epoch 374/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 19.0507 - val_loss: 19.1412\n",
      "Epoch 375/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 19.0256 - val_loss: 19.1160\n",
      "Epoch 376/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 19.0006 - val_loss: 19.0909\n",
      "Epoch 377/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 18.9756 - val_loss: 19.0659\n",
      "Epoch 378/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.9507 - val_loss: 19.0411\n",
      "Epoch 379/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.9258 - val_loss: 19.0161\n",
      "Epoch 380/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.9010 - val_loss: 18.9913\n",
      "Epoch 381/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 18.8761 - val_loss: 18.9664\n",
      "Epoch 382/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.8514 - val_loss: 18.9417\n",
      "Epoch 383/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 18.8268 - val_loss: 18.9170\n",
      "Epoch 384/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.8021 - val_loss: 18.8922\n",
      "Epoch 385/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.7775 - val_loss: 18.8674\n",
      "Epoch 386/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.7529 - val_loss: 18.8428\n",
      "Epoch 387/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.7284 - val_loss: 18.8183\n",
      "Epoch 388/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 18.7039 - val_loss: 18.7938\n",
      "Epoch 389/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 18.6795 - val_loss: 18.7693\n",
      "Epoch 390/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 18.6551 - val_loss: 18.7450\n",
      "Epoch 391/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.6308 - val_loss: 18.7207\n",
      "Epoch 392/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 18.6065 - val_loss: 18.6964\n",
      "Epoch 393/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 18.5823 - val_loss: 18.6721\n",
      "Epoch 394/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.5581 - val_loss: 18.6478\n",
      "Epoch 395/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.5339 - val_loss: 18.6236\n",
      "Epoch 396/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 18.5098 - val_loss: 18.5994\n",
      "Epoch 397/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 18.4858 - val_loss: 18.5754\n",
      "Epoch 398/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.4618 - val_loss: 18.5515\n",
      "Epoch 399/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.4379 - val_loss: 18.5274\n",
      "Epoch 400/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 18.4140 - val_loss: 18.5034\n",
      "Epoch 401/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 18.3900 - val_loss: 18.4794\n",
      "Epoch 402/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 18.3662 - val_loss: 18.4553\n",
      "Epoch 403/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 18.3423 - val_loss: 18.4312\n",
      "Epoch 404/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 18.3184 - val_loss: 18.4072\n",
      "Epoch 405/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 18.2946 - val_loss: 18.3834\n",
      "Epoch 406/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 18.2709 - val_loss: 18.3597\n",
      "Epoch 407/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 18.2473 - val_loss: 18.3361\n",
      "Epoch 408/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 18.2237 - val_loss: 18.3126\n",
      "Epoch 409/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 18.2002 - val_loss: 18.2891\n",
      "Epoch 410/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 18.1767 - val_loss: 18.2656\n",
      "Epoch 411/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.1533 - val_loss: 18.2422\n",
      "Epoch 412/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 18.1299 - val_loss: 18.2188\n",
      "Epoch 413/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.1066 - val_loss: 18.1953\n",
      "Epoch 414/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 18.0832 - val_loss: 18.1719\n",
      "Epoch 415/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 18.0599 - val_loss: 18.1486\n",
      "Epoch 416/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.0367 - val_loss: 18.1254\n",
      "Epoch 417/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 18.0135 - val_loss: 18.1021\n",
      "Epoch 418/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.9904 - val_loss: 18.0790\n",
      "Epoch 419/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 17.9672 - val_loss: 18.0559\n",
      "Epoch 420/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 17.9442 - val_loss: 18.0328\n",
      "Epoch 421/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 17.9211 - val_loss: 18.0097\n",
      "Epoch 422/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 17.8981 - val_loss: 17.9867\n",
      "Epoch 423/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.8751 - val_loss: 17.9637\n",
      "Epoch 424/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.8521 - val_loss: 17.9407\n",
      "Epoch 425/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 17.8293 - val_loss: 17.9178\n",
      "Epoch 426/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 17.8064 - val_loss: 17.8948\n",
      "Epoch 427/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.7836 - val_loss: 17.8718\n",
      "Epoch 428/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.7608 - val_loss: 17.8488\n",
      "Epoch 429/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.7380 - val_loss: 17.8259\n",
      "Epoch 430/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 17.7153 - val_loss: 17.8031\n",
      "Epoch 431/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 17.6927 - val_loss: 17.7805\n",
      "Epoch 432/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 17.6701 - val_loss: 17.7578\n",
      "Epoch 433/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 17.6475 - val_loss: 17.7353\n",
      "Epoch 434/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.6249 - val_loss: 17.7126\n",
      "Epoch 435/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 17.6024 - val_loss: 17.6900\n",
      "Epoch 436/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 17.5799 - val_loss: 17.6675\n",
      "Epoch 437/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 17.5574 - val_loss: 17.6451\n",
      "Epoch 438/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.5351 - val_loss: 17.6227\n",
      "Epoch 439/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 17.5127 - val_loss: 17.6004\n",
      "Epoch 440/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.4905 - val_loss: 17.5781\n",
      "Epoch 441/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.4681 - val_loss: 17.5558\n",
      "Epoch 442/500\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 17.4459 - val_loss: 17.5334\n",
      "Epoch 443/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 17.4237 - val_loss: 17.5113\n",
      "Epoch 444/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 17.4015 - val_loss: 17.4891\n",
      "Epoch 445/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 17.3794 - val_loss: 17.4669\n",
      "Epoch 446/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 33us/sample - loss: 17.3572 - val_loss: 17.4447\n",
      "Epoch 447/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 17.3351 - val_loss: 17.4225\n",
      "Epoch 448/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 17.3131 - val_loss: 17.4004\n",
      "Epoch 449/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 17.2911 - val_loss: 17.3784\n",
      "Epoch 450/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 17.2691 - val_loss: 17.3564\n",
      "Epoch 451/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 17.2472 - val_loss: 17.3343\n",
      "Epoch 452/500\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 17.2252 - val_loss: 17.3123\n",
      "Epoch 453/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.2034 - val_loss: 17.2904\n",
      "Epoch 454/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 17.1816 - val_loss: 17.2685\n",
      "Epoch 455/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.1598 - val_loss: 17.2468\n",
      "Epoch 456/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 17.1380 - val_loss: 17.2249\n",
      "Epoch 457/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 17.1162 - val_loss: 17.2032\n",
      "Epoch 458/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 17.0945 - val_loss: 17.1814\n",
      "Epoch 459/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 17.0728 - val_loss: 17.1595\n",
      "Epoch 460/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 17.0512 - val_loss: 17.1378\n",
      "Epoch 461/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 17.0296 - val_loss: 17.1161\n",
      "Epoch 462/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 17.0080 - val_loss: 17.0944\n",
      "Epoch 463/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 16.9864 - val_loss: 17.0728\n",
      "Epoch 464/500\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 16.9650 - val_loss: 17.0512\n",
      "Epoch 465/500\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 16.9435 - val_loss: 17.0298\n",
      "Epoch 466/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 16.9220 - val_loss: 17.0082\n",
      "Epoch 467/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 16.9005 - val_loss: 16.9866\n",
      "Epoch 468/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 16.8791 - val_loss: 16.9650\n",
      "Epoch 469/500\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 16.8578 - val_loss: 16.9436\n",
      "Epoch 470/500\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 16.8365 - val_loss: 16.9221\n",
      "Epoch 471/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.8151 - val_loss: 16.9007\n",
      "Epoch 472/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.7939 - val_loss: 16.8794\n",
      "Epoch 473/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.7727 - val_loss: 16.8581\n",
      "Epoch 474/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.7515 - val_loss: 16.8369\n",
      "Epoch 475/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 16.7304 - val_loss: 16.8157\n",
      "Epoch 476/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 16.7093 - val_loss: 16.7945\n",
      "Epoch 477/500\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 16.6881 - val_loss: 16.7733\n",
      "Epoch 478/500\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 16.6671 - val_loss: 16.7522\n",
      "Epoch 479/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.6461 - val_loss: 16.7310\n",
      "Epoch 480/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.6250 - val_loss: 16.7100\n",
      "Epoch 481/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.6041 - val_loss: 16.6891\n",
      "Epoch 482/500\n",
      "1168/1168 [==============================] - 0s 31us/sample - loss: 16.5831 - val_loss: 16.6680\n",
      "Epoch 483/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 16.5622 - val_loss: 16.6470\n",
      "Epoch 484/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.5413 - val_loss: 16.6260\n",
      "Epoch 485/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.5204 - val_loss: 16.6051\n",
      "Epoch 486/500\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 16.4996 - val_loss: 16.5842\n",
      "Epoch 487/500\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 16.4788 - val_loss: 16.5633\n",
      "Epoch 488/500\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 16.4581 - val_loss: 16.5424\n",
      "Epoch 489/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.4373 - val_loss: 16.5217\n",
      "Epoch 490/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 16.4167 - val_loss: 16.5010\n",
      "Epoch 491/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.3960 - val_loss: 16.4802\n",
      "Epoch 492/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.3753 - val_loss: 16.4595\n",
      "Epoch 493/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.3547 - val_loss: 16.4389\n",
      "Epoch 494/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.3342 - val_loss: 16.4183\n",
      "Epoch 495/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 16.3136 - val_loss: 16.3978\n",
      "Epoch 496/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.2931 - val_loss: 16.3772\n",
      "Epoch 497/500\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 16.2726 - val_loss: 16.3567\n",
      "Epoch 498/500\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 16.2521 - val_loss: 16.3362\n",
      "Epoch 499/500\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 16.2317 - val_loss: 16.3158\n",
      "Epoch 500/500\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 16.2113 - val_loss: 16.2953\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_features, \n",
    "                  train_labels,\n",
    "                  validation_data=(valid_features, valid_labels), \n",
    "                  epochs=200, \n",
    "                  batch_size=128,\n",
    "                  validation_freq=1,\n",
    "                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='valid')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用全量数据再训练一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tf.convert_to_tensor(train_features_all,dtype=tf.float32)\n",
    "y_train=tf.convert_to_tensor(train_labels_all,dtype=tf.float32)\n",
    "x_test=tf.convert_to_tensor(test_features,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb397677b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "adam=tf.keras.optimizers.Adam(0.5)\n",
    "model.compile(optimizer=adam,\n",
    "              loss=tf.keras.losses.mean_squared_logarithmic_error\n",
    "              )\n",
    "model.fit(x_train, y_train, epochs=200,batch_size=32,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10871.305 ],\n",
       "       [ 9857.99  ],\n",
       "       [11197.828 ],\n",
       "       [11669.458 ],\n",
       "       [12823.4795],\n",
       "       [11259.658 ],\n",
       "       [11731.108 ],\n",
       "       [11424.263 ],\n",
       "       [11436.016 ],\n",
       "       [10614.987 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model.predict(x_test)  # 预测结果\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "回归问题的输出层无法使用activation吗？\n",
    "\n",
    "tf.keras.layers.Dense(1) 这是什么意思？ 就是只有1个神经元呗？\n",
    "\n",
    "如果按照这个逻辑，那二分类问题，只有一个输出单元就可以了？\n",
    "\n",
    "均方根误差，对数均方根误差的区别？ 可以根据函数曲线的形状来分析两者的区别，比如有的函数对异常值不敏感。\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
